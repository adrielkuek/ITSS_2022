{"cells":[{"cell_type":"markdown","metadata":{"id":"-x-x2hhlOKkl"},"source":["# Violence Detection using CNN + LSTM neural network"]},{"cell_type":"markdown","metadata":{"id":"UVXa7uMnOlkp"},"source":["## Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24395,"status":"ok","timestamp":1652259868614,"user":{"displayName":"PL Yap","userId":"04862772682578404355"},"user_tz":-480},"id":"KkuFPILRzX2K","outputId":"bc19fd2c-fd92-4dd8-eb73-c4ec2a774894"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","/content/gdrive/MyDrive/Practice Module/Violence-Detection\n","/content/gdrive/MyDrive/Practice Module/Violence-Detection\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","import os\n","%cd '/content/gdrive/MyDrive/Practice Module/Violence-Detection'\n","pwd = os.getcwd()\n","print(pwd)"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"oDfDnlliPMd-","executionInfo":{"status":"ok","timestamp":1652259871565,"user_tz":-480,"elapsed":2956,"user":{"displayName":"PL Yap","userId":"04862772682578404355"}}},"outputs":[],"source":["%matplotlib inline\n","import cv2\n","import os\n","import glob\n","import random\n","import sys\n","import h5py\n","import numpy as np\n","# import matplotlib.pyplot as plt\n","# from random import shuffle\n","# from sklearn.metrics import confusion_matrix, accuracy_score\n","\n","import tensorflow as tf\n","from tensorflow.keras.callbacks import ModelCheckpoint,CSVLogger\n","from tensorflow.keras.applications import VGG16\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras.layers import Input\n","from tensorflow.keras.layers import LSTM\n","from tensorflow.keras.layers import Dense, Activation , Dropout\n"]},{"cell_type":"markdown","metadata":{"id":"v6pf1l28PzIO"},"source":["## Helper Functions"]},{"cell_type":"markdown","metadata":{"id":"WKnyJkf8PzxE"},"source":["We will use the function ```print_progress``` to print the amount of videos processed the datasets"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"qnafWmS7P3CG","executionInfo":{"status":"ok","timestamp":1652259872924,"user_tz":-480,"elapsed":2,"user":{"displayName":"PL Yap","userId":"04862772682578404355"}}},"outputs":[],"source":["def print_progress(count, max_count):\n","    # Percentage completion.\n","    pct_complete = count / max_count\n","\n","    # Status-message. Note the \\r which means the line should overwrite itself.\n","    msg = \"\\r- Progress: {0:.1%}\".format(pct_complete)\n","\n","    sys.stdout.write(msg)\n","    sys.stdout.flush()"]},{"cell_type":"markdown","metadata":{"id":"tRf-KgkjP9Kt"},"source":["## Load Data\n","\n","Firstly, we define the directory to place the video dataset"]},{"cell_type":"markdown","metadata":{"id":"UCZHrpJjRKky"},"source":["Copy some of the data-dimensions for convenience."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"SXTNEj6SRLZZ","executionInfo":{"status":"ok","timestamp":1652259876823,"user_tz":-480,"elapsed":562,"user":{"displayName":"PL Yap","userId":"04862772682578404355"}}},"outputs":[],"source":["# Frame size  \n","img_size = 224\n","\n","img_size_tuple = (img_size, img_size)\n","\n","# Number of channels (RGB)\n","num_channels = 3\n","\n","# Flat frame size\n","img_size_flat = img_size * img_size * num_channels\n","\n","# Number of classes for classification (Violence-No Violence)\n","num_classes = 2\n","\n","# Number of files to train\n","_num_files_train = 1\n","\n","# Number of frames per video\n","_images_per_file = 100\n","\n","# Number of frames per training set\n","_num_images_train = _num_files_train * _images_per_file\n","\n","# Video extension\n","video_exts = \".avi\""]},{"cell_type":"markdown","metadata":{"id":"Wodq7EaSRSS8"},"source":["#### Helper-function for getting video frames\n","Function used to get 100 frames from a video file and convert the frame to a suitable format for the neural net."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"eu9c4a-3RVkO","executionInfo":{"status":"ok","timestamp":1652259877930,"user_tz":-480,"elapsed":3,"user":{"displayName":"PL Yap","userId":"04862772682578404355"}}},"outputs":[],"source":["def get_frames(current_dir, file_name):\n","    \n","    in_file = os.path.join(current_dir, file_name)\n","    \n","    images = []\n","    \n","    vidcap = cv2.VideoCapture(in_file)\n","    \n","    success,image = vidcap.read()\n","        \n","    count = 0\n","\n","    while count<_images_per_file:\n","                \n","        RGB_img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    \n","        res = cv2.resize(RGB_img, dsize=(img_size, img_size),\n","                                 interpolation=cv2.INTER_CUBIC)\n","    \n","        images.append(res)\n","    \n","        success,image = vidcap.read()\n","    \n","        count += 1\n","        \n","    resul = np.array(images)\n","    \n","    resul = (resul / 255.).astype(np.float16)\n","        \n","    return resul"]},{"cell_type":"markdown","metadata":{"id":"tLCjYFBtRZb-"},"source":["#### Helper function to get the names of the data downloaded and label it"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"Qiv5NIJjRbIA","executionInfo":{"status":"ok","timestamp":1652259880926,"user_tz":-480,"elapsed":357,"user":{"displayName":"PL Yap","userId":"04862772682578404355"}}},"outputs":[],"source":["def label_video_names(in_dir, sample_size=100):\n","    \n","    # list containing video names\n","    names = []\n","    # list containin video labels [1, 0] if it has violence and [0, 1] if not\n","    labels = []\n","    \n","    \n","    for current_dir, dir_names,file_names in os.walk(in_dir):\n","        \n","        for file_name in file_names:\n","            \n","            if file_name[0:2] == 'fi':\n","                labels.append([1,0])\n","                names.append(file_name)\n","            elif file_name[0:2] == 'no':\n","                labels.append([0,1])\n","                names.append(file_name)\n","                     \n","            \n","    c = list(zip(names,labels))\n","    # Suffle the data (names and labels)\n","    res = random.sample(c, sample_size)\n","    \n","    names, labels = zip(*res)\n","            \n","    return names, labels"]},{"cell_type":"markdown","metadata":{"id":"sF1QieG5SANp"},"source":["#### Pre-Trained Model: VGG16"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"MjRN6oE4SC81","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652259892464,"user_tz":-480,"elapsed":8847,"user":{"displayName":"PL Yap","userId":"04862772682578404355"}},"outputId":"f4448639-e4a4-4066-c833-ac4f72170107"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n","553467904/553467096 [==============================] - 3s 0us/step\n","553476096/553467096 [==============================] - 3s 0us/step\n"]}],"source":["vid_model = VGG16(include_top=True, weights='imagenet')"]},{"cell_type":"markdown","metadata":{"id":"ud7OU0t7SQPi"},"source":["vid_model.summary()\n"]},{"cell_type":"markdown","metadata":{"id":"-NxnGLlDSTwr"},"source":["We can observe the shape of the tensors expected as input by the pre-trained VGG16 model. In this case it is images of shape 224 x 224 x 3. Note that we have defined the frame size as 224x224x3. The video frame will be the input of the VGG16 net."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":594,"status":"ok","timestamp":1652259895832,"user":{"displayName":"PL Yap","userId":"04862772682578404355"},"user_tz":-480},"id":"4YWFA-2tSdfB","outputId":"6f9652a5-e1bc-4c26-8f37-7f54b1727971"},"outputs":[{"output_type":"stream","name":"stdout","text":["The input of the VGG16 net have dimensions: (224, 224)\n","The output of the selecter layer of VGG16 net have dimensions:  4096\n"]}],"source":["# We will use the output of the layer prior to the final\n","# classification-layer which is named fc2. This is a fully-connected (or dense) layer.\n","transfer_layer = vid_model.get_layer('fc2')\n","\n","image_model_transfer = Model(inputs=vid_model.input,\n","                             outputs=transfer_layer.output)\n","\n","transfer_values_size = K.int_shape(transfer_layer.output)[1]\n","\n","\n","print(\"The input of the VGG16 net have dimensions:\",K.int_shape(vid_model.input)[1:3])\n","\n","print(\"The output of the selecter layer of VGG16 net have dimensions: \", transfer_values_size)"]},{"cell_type":"markdown","metadata":{"id":"HGdRIG6oSooG"},"source":["#### Generator that process one video through VGG16 each function call"]},{"cell_type":"code","source":["in_dir = '../RWF-2000/train'"],"metadata":{"id":"qejeOuzjVFfL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I-Cnv0fRSswF"},"source":["### Functions to save transfer values from VGG16 to later use\n","We are going to define functions to get the transfer values from VGG16 with defined number of files. Then save the transfer values files used from training in one file and the ones uses for testing in another one. "]},{"cell_type":"code","source":["def process_transfer(vid_names, in_dir, labels):\n","    \n","    count = 0\n","    \n","    tam = len(vid_names)\n","    \n","    # Pre-allocate input-batch-array for images.\n","    shape = (_images_per_file,) + img_size_tuple + (3,)\n","    \n","    while count<tam:\n","        \n","        video_name = vid_names[count]\n","        \n","        image_batch = np.zeros(shape=shape, dtype=np.float16)\n","        \n","        try:\n","            image_batch = get_frames(in_dir, video_name)\n","        except Exception as e:\n","          print('Exception Error :', e, video_name)\n","          count += 1\n","          continue\n","        \n","        # Note that we use 16 bit floating pt to save memory\n","        shape = (_images_per_file, transfer_values_size)\n","        transfer_values = np.zeros(shape=shape, dtype=np.float16)\n","        \n","        transfer_values = image_model_transfer.predict(image_batch)\n","         \n","        labels1 = labels[count]\n","        \n","        aux = np.ones([100,2])\n","        \n","        labelss = labels1*aux\n","        \n","        yield transfer_values, labelss\n","        \n","        count+=1"],"metadata":{"id":"Q5KwSjC9XpTg","executionInfo":{"status":"ok","timestamp":1652260196982,"user_tz":-480,"elapsed":680,"user":{"displayName":"PL Yap","userId":"04862772682578404355"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","execution_count":17,"metadata":{"id":"tvU53ypSSvL0","executionInfo":{"status":"ok","timestamp":1652260200691,"user_tz":-480,"elapsed":2,"user":{"displayName":"PL Yap","userId":"04862772682578404355"}}},"outputs":[],"source":["def make_files(file_names, file_labels, output_file):\n","    \n","    gen = process_transfer(file_names, in_dir, file_labels)\n","\n","    numer = 1\n","    n_files = len(file_names)\n","\n","    # Read the first chunk to get the column dtypes\n","    chunk = next(gen)\n","\n","    row_count = chunk[0].shape[0]\n","    row_count2 = chunk[1].shape[0]\n","    \n","    with h5py.File(output_file, 'w') as f:\n","    \n","        # Initialize a resizable dataset to hold the output\n","        maxshape = (None,) + chunk[0].shape[1:]\n","        maxshape2 = (None,) + chunk[1].shape[1:]\n","     \n","        dset = f.create_dataset('data', shape=chunk[0].shape, maxshape=maxshape,\n","                                chunks=chunk[0].shape, dtype=chunk[0].dtype)\n","    \n","        dset2 = f.create_dataset('labels', shape=chunk[1].shape, maxshape=maxshape2,\n","                                 chunks=chunk[1].shape, dtype=chunk[1].dtype)\n","    \n","         # Write the first chunk of rows\n","        dset[:] = chunk[0]\n","        dset2[:] = chunk[1]\n","\n","        for chunk in gen:\n","            \n","            if numer == n_files:\n","            \n","                break\n","\n","            # Resize the dataset to accommodate the next chunk of rows\n","            dset.resize(row_count + chunk[0].shape[0], axis=0)\n","            dset2.resize(row_count2 + chunk[1].shape[0], axis=0)\n","\n","            # Write the next chunk\n","            dset[row_count:] = chunk[0]\n","            dset2[row_count:] = chunk[1]\n","\n","            # Increment the row count\n","            row_count += chunk[0].shape[0]\n","            row_count2 += chunk[1].shape[0]\n","            \n","            print_progress(numer, n_files)\n","        \n","            numer += 1"]},{"cell_type":"markdown","metadata":{"id":"R9axnZ8dS64T"},"source":["#### Split the dataset into training set and test set"]},{"cell_type":"code","source":["in_dir = \"../keypoints/val\"\n","fight, no_fight = [], []\n","for current_dir, dir_names, file_names in os.walk(in_dir):\n","  for file_name in file_names:\n","    if file_name[0:2] == 'fi':\n","      fight.append(file_name)\n","    else:\n","      no_fight.append(file_name)\n","\n","print(len(fight), len(no_fight))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qc2Yt4RgX_jz","executionInfo":{"status":"ok","timestamp":1652262955581,"user_tz":-480,"elapsed":4793,"user":{"displayName":"PL Yap","userId":"04862772682578404355"}},"outputId":"f4e55ce3-c925-4e19-d079-c187689d6a37"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["184 111\n"]}]},{"cell_type":"code","execution_count":14,"metadata":{"id":"wjne3svdS9Y3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652260134231,"user_tz":-480,"elapsed":428,"user":{"displayName":"PL Yap","userId":"04862772682578404355"}},"outputId":"c67d7bb9-099b-4809-ad35-2389b7fdf949"},"outputs":[{"output_type":"stream","name":"stdout","text":["886 886 222 222\n"]}],"source":["#tr_dir = '../RWF-2000/train'\n","#in_dir = \"../opt_flow/train\"\n","in_dir = \"../keypoints/train\"\n","names, labels = label_video_names(in_dir, sample_size=1108)\n","\n","test_set = int(len(names)*0.2)\n","train_set = int(len(names)*0.8)\n","\n","names_train = names[0:train_set]\n","names_test = names[train_set:]\n","\n","labels_train = labels[0:train_set]\n","labels_test = labels[train_set:]\n","\n","print(len(names_train), len(labels_train), len(names_test), len(labels_test))"]},{"cell_type":"code","source":["make_files(names_train, labels_train, 'datasets/lstm_keypoints_train.h5')"],"metadata":{"id":"pyO9WP-6TER4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652261434137,"user_tz":-480,"elapsed":1225085,"user":{"displayName":"PL Yap","userId":"04862772682578404355"}},"outputId":"52c8ee64-e7f5-46d2-fb61-b5354118f121"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["- Progress: 99.9%"]}]},{"cell_type":"code","source":["make_files(names_test, labels_test, 'datasets/lstm_keypoints_test.h5')"],"metadata":{"id":"0ThoefXUXPrS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652261742263,"user_tz":-480,"elapsed":308132,"user":{"displayName":"PL Yap","userId":"04862772682578404355"}},"outputId":"0163b475-0c6c-4230-effe-d635e2d9f9f3"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["- Progress: 99.5%"]}]},{"cell_type":"code","source":["# Process validation files\n","#val_dir = '../RWF-2000/val'\n","#in_dir = '../opt_flow/val'\n","in_dir = '../keypoints/val'\n","names_val, labels_val = label_video_names(in_dir, sample_size=295)\n","make_files(names_val, labels_val, 'datasets/lstm_keypoints_val.h5')\n","print(len(names_val), len(labels_val))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kk8q753Y98P1","executionInfo":{"status":"ok","timestamp":1652263224811,"user_tz":-480,"elapsed":257277,"user":{"displayName":"PL Yap","userId":"04862772682578404355"}},"outputId":"655864f7-f272-4eb6-dcf6-590d2953f42f"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["- Progress: 99.7%295 295\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"jh3VwGXGRuTz"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Violent_Detection_CNN+LSTM_Data_Preparation.ipynb","provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":0}